<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2022-05-21T05:22:41+09:00</updated><id>/feed.xml</id><title type="html">No pain, No gain</title><subtitle>Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.</subtitle><author><name>innerpyg</name></author><entry><title type="html">Classification with CNN, LSTM using DNA sequence data</title><link href="/bioinformatics/2022/05/21/BICase1.html" rel="alternate" type="text/html" title="Classification with CNN, LSTM using DNA sequence data" /><published>2022-05-21T00:00:00+09:00</published><updated>2022-05-21T00:00:00+09:00</updated><id>/bioinformatics/2022/05/21/BICase1</id><content type="html" xml:base="/bioinformatics/2022/05/21/BICase1.html"><![CDATA[<h2 id="bi에서의-딥러닝-활용">BI에서의 딥러닝 활용</h2>

<p><br /></p>

<h4 id="--paper-title">- Paper title</h4>

<p><img src="/assets/images/CV/BICase-1.png" alt="" /></p>

<p><br /></p>

<ul>
  <li>머신러닝, 딥러닝 등 여러 분석 기법을 배우고 있지만 실무적으로 어떻게 쓰일 수 있는지 예시를 보여주기 위함.</li>
  <li>실무적으로 다룰 데이터가 어떤 특성을 가지는 것인지 즉, 도메인의 지식이 중요하다는 것을 알 수 있음.</li>
</ul>

<p><br /><br /></p>

<h4 id="--sample-status">- Sample Status</h4>

<p><img src="/assets/images/CV/BICase-2.png" alt="" /></p>

<p><br /><br /></p>

<h4 id="--classification-workflow">- Classification workflow</h4>

<p><img src="/assets/images/CV/BICase-3.png" alt="" /></p>

<p><br /><br /></p>

<h4 id="--process">- Process</h4>

<ol>
  <li>Data Collection
    <ul>
      <li>모든 샘플은 NCBI에서 확보</li>
      <li>class imbalance하게 때문에 SMOTE 적용</li>
    </ul>
  </li>
  <li>Data pre-processing
    <ul>
      <li>DNA sequence는 문자열 데이터이기 때문에 수치형 데이터로 변환하는 encoding 작업을 진행</li>
    </ul>
    <ol>
      <li>Label encoding</li>
      <li>K-mer encoding(k-means 아님)</li>
    </ol>

    <p><img src="/assets/images/CV/BICase-4.png" width="300" height="200" /></p>
  </li>
  <li>Classification model
    <ul>
      <li>CNN</li>
      <li>CNN + LSTM</li>
      <li>CNN + Bidirectional LSTM</li>
    </ul>
  </li>
</ol>

<p><br /><br /></p>

<h4 id="--accuracy-of-classification-models">- Accuracy of classification models</h4>

<p><img src="/assets/images/CV/BICase-5.png" alt="" /></p>

<p><br /><br /></p>

<h4 id="--conclusion">- Conclusion</h4>

<ul>
  <li>이 논문의 목적은 사실상 DNA sequence를 이용하여 species를 잘 classification 할 수 있는지, 그리고 정확도는 좋은지 확인하는 목적</li>
  <li>그러나 BI 실무 및 딥러닝 입문자의 입장에서는 딥러닝 기술을 이용하여 meta genome(여러 genome이 모여있는 형태)에 대해 각 species별 분류가 가능하고, 더 나아가 아직 밝혀지지 않은 sequence 정보를 확보 후 신약 개발에 응용한다는 등 여러 가능성이 보임.(물론 이미 많은 개발이 이루어져있겠지..)</li>
</ul>

<p><br /><br /></p>

<p><strong>위 내용들은 모두 아래의 논문을 참고하였음</strong></p>

<p><a href="https://www.hindawi.com/journals/cmmm/2021/1835056/">Analysis of DNA Sequence Classification Using CNN and Hybrid Models</a></p>]]></content><author><name>Written by Aaron</name></author><category term="Bioinformatics" /><category term="Bioinformatics" /><category term="CNN" /><category term="LSTM" /><summary type="html"><![CDATA[BI에서의 딥러닝 활용]]></summary></entry><entry><title type="html">YOLO</title><link href="/computervision/2022/05/21/Chapter4.html" rel="alternate" type="text/html" title="YOLO" /><published>2022-05-21T00:00:00+09:00</published><updated>2022-05-21T00:00:00+09:00</updated><id>/computervision/2022/05/21/Chapter4</id><content type="html" xml:base="/computervision/2022/05/21/Chapter4.html"><![CDATA[<p><img src="/assets/images/CV/YOLO-3.png" alt="" /></p>

<p><br /><br /></p>

<h2 id="1-yolo---v1">1. YOLO - V1</h2>

<p><br /></p>

<ul>
  <li>input image를 S x S로 나눈 grid의 각각의 cell 별로 하나의 object에 대한 detection 수행</li>
  <li>각 cell이 2개의 Bounding box를 기반으로 하나의 object에 대한 bounding box 예측</li>
</ul>

<p><img src="/assets/images/CV/YOLO-1.png" alt="" /></p>

<p><br /></p>

<ul>
  <li>input image는 반복되는 C.R &gt; dense layer(classification, regression) &gt; output feature map( S x S grid )</li>
  <li>S x S grid가 갖는 channel은 다음의 특징을 가질 것으로 추정
    <ul>
      <li>사용하는 데이터 셋마다 다름(예측하는 object 종류에 따라)
        <ol>
          <li>2개의 bounding box에 대한 좌표</li>
          <li>Confidence Score = probability of object( not class )</li>
          <li>예측하고자 하는 각 class 별 확률</li>
        </ol>
      </li>
    </ul>
  </li>
</ul>

<blockquote>
  <p>PASCAL VOC 데이터 셋인 경우 예측하고자 하는 class가 20개이기 때문에, channel은 5*2+20</p>
</blockquote>

<p><br /></p>

<h4 id="loss-function">Loss function</h4>

<p><br /></p>

<ol>
  <li>BBox central X,Y loss</li>
</ol>

<ul>
  <li>예측 좌표 x,y값과 Ground Truth x,y 오차 제곱 기반</li>
  <li>모든 cell * 2개의 BBox 중 대표되는 BBox만 계산</li>
</ul>

<ol>
  <li>BBox width, height loss</li>
</ol>

<ul>
  <li>x,y와 동일하지만 오차 제곱을 기반으로 계산</li>
  <li>크기에 따라 error가 커지는 것을 제한하기 위해 제곱근 사용</li>
</ul>

<ol>
  <li>Object Confidence loss</li>
</ol>

<ul>
  <li>Ground Truth 와의 Object Confidence Score의 IoU 기반 예측 오차</li>
  <li>각 cell 당 2개의 BBox모두 확인</li>
</ul>

<ol>
  <li>Classification loss</li>
</ol>

<ul>
  <li>대표되는 BBox에 대해 object class 별 확률 오차의 제곱</li>
</ul>

<p><br /></p>

<p><img src="/assets/images/CV/YOLO-2.png" alt="" /></p>

<p><br /></p>

<h4 id="one-stage-detector의-경우-대부분-nms로-마무리-하는-경향이-있음">One stage detector의 경우 대부분 NMS로 마무리 하는 경향이 있음.</h4>
<h4 id="nms-종류">NMS 종류</h4>

<ol>
  <li>Greedy NMS : <a href="https://innerpyg.github.io/computervision/2022/04/30/Chapter1.html">Object Detection과 Segmentation</a>에서 배웠던 내용처럼, 높은 Confidence score를 가진 BBox을 뽑고, Ground Truth와의 IoU가 낮은 BBox를 제거하는 가장 일반적인 방법.</li>
  <li>Soft NMS : YOLO - V1에 적용된 NMS 방법으로 Confidence score가 가장 높은 BBox를 시작으로 겹치는 BBox 간의 IoU가 특정 threshold 이상 값을 나타낼 때, 해당 BBox를 제거하는 방법.</li>
  <li>DIoU(Distance IoU) NMS : 두 BBox의 중심점의 거리를 고려</li>
  <li>GIoU, CIoU</li>
</ol>

<p><br /></p>

<p><strong>YOLO - V1은 속도는 빨라졌으나, 성능 특히 작은 object detection 성능이 좋지 않다.</strong></p>

<p><br /><br /></p>

<h2 id="2-yolo---v2">2. YOLO - V2</h2>

<p><br /></p>

<p><img src="/assets/images/CV/YOLO-4.png" alt="" /></p>

<p><strong>지금까지는 Feature Extract &gt; Classification layer(dense)의 형태였는데, YOLO-V2에서는 Classification시 dense가 아닌 Fully Conv layer로 변경</strong>
<strong>학습 시 Feature Extract 부분은 freezing하여 weight를 업데이트 하지 않도록 하고, Fully Conv만 weight를 업데이트</strong>
<strong>input image의 크기는 32배수가 아니면 feature map size 문제로 진행할 수 없다고 이해.</strong></p>

<p><strong>여기서 한가지 의문은… 서로 다른 크기의 image들을 학습하는 상황에서 만약 dense를 쓴다면 서로 다른 크기의 이미지들로 부터 생성되는 dense의 사이즈가 달라지기 때문에 dense를 할 수 없다고 하는 것 같은데… <span style="color:red"> flatten layer로 dense를 만들 때 SPP같은 걸 쓰면 서로 다른 크기의 이미지라도 같은 크기의 1D dense layer를 만들 수 있는 거 아닌가?</span></strong></p>

<p><br /></p>

<h4 id="output-feature-map">Output Feature Map</h4>

<p><br /></p>

<p><img src="/assets/images/CV/YOLO-5.png" alt="" /></p>

<p><br /></p>

<h4 id="direct-location-prediction">Direct location prediction</h4>

<p><br /></p>

<p><img src="/assets/images/CV/YOLO-6.png" alt="" /></p>

<p><br /></p>

<ul>
  <li>RCNN에서도 bounding box regression을 진행했었으나, 그대로 진행하게 되면 현재 각 cell 단위로 진행하는 시점에서는 기준이 되는 cell의 범위를 벗어난 곳에 bounding box가 형성될 수도 있음. 특히 학습이 제대로 되어있지 않은 초기에는 문제가 심각할 것으로 보임.</li>
  <li>그렇기 때문에 각 cell을 기준 좌상단 점으로 부터 cell을 벗어나지 않는 곳에 중심점을 잡고 predicted bounding box가 형성되기 하기 위해서는 sigmoid function을 적용하여 width와 height가 0 ~ 1범위 내에 존재하게 하는 것이 중요.</li>
</ul>

<p><br /></p>

<h4 id="passthrough-module-적용">Passthrough module 적용</h4>

<p><br />
<img src="/assets/images/CV/YOLO-7.png" alt="" /></p>

<ul>
  <li>작은 object를 detect하기 위해 다소 큰 feature map을 reshape함으로써 성능을 보정</li>
  <li>앞서 <a href="https://innerpyg.github.io/computervision/2022/05/19/Chapter3.html">SDD</a>에서 multi-scale feature map을 이용하여 크고 작은 object를 detection하려는 것과 같이, 특정 시점의 feature map의 특성을 단순히 reshape함으로써 좀 더 작은 object를 detect하기 위함.</li>
</ul>]]></content><author><name>Written by Aaron</name></author><category term="ComputerVision" /><category term="ComputerVision" /><category term="SSD" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">SSD</title><link href="/computervision/2022/05/19/Chapter3.html" rel="alternate" type="text/html" title="SSD" /><published>2022-05-19T00:00:00+09:00</published><updated>2022-05-19T00:00:00+09:00</updated><id>/computervision/2022/05/19/Chapter3</id><content type="html" xml:base="/computervision/2022/05/19/Chapter3.html"><![CDATA[<h2 id="ssdsingle-shot-multibox-detector">SSD(Single Shot multibox Detector)</h2>

<p><br /></p>

<h4 id="background">background</h4>

<p><img src="/assets/images/CV/SSD-1.png" alt="SSD-1" /></p>

<ul>
  <li>one stage vs two stage : Resion proposal이 별도로 구성되어있느냐 없느냐의 차이.</li>
  <li>two stage detector 중 Faster RCNN이 비교적 빠른 속도임에도 불구하고 실시간으로 사용하기에는 무리가 있음.</li>
  <li>object detection의 inference time을 줄이기 위해 one stage에 대해 학습.</li>
  <li>YOLO로 인해 inference time은 향상되었으나 mAP가 떨어짐.</li>
  <li>뒤이어 나온 SSD가 inference time도 좋고 mAP가 Faster RCNN보다 좋은 모델.</li>
</ul>

<p><br /><br /></p>

<h4 id="ssd-주요-구성-요소">SSD 주요 구성 요소</h4>

<h5 id="--feature-map-기반-multi-scale-feature-layer">- <strong>Feature Map 기반 Multi Scale Feature Layer</strong></h5>

<p><br /></p>

<p><img src="/assets/images/CV/SSD-2.png" alt="SSD-2" /></p>

<ul>
  <li>Sliding Window를 기반으로 응용</li>
  <li>큰 feature map의 경우 원본 이미지에 가까운 특징들을 가지고 있고, 비교적 작은 사이즈의 Object를 detect하기 용이.</li>
  <li>거듭 CNN을 진행한 더 작은 feature map의 경우 더 추상적이고 핵심적인 특징들을 가지고 있고, 비교적 큰 사이즈의 Object를 detect하기 용이.</li>
</ul>

<p><br /><br /></p>

<h5 id="--default-anchor-box">- <strong>Default (Anchor) Box</strong></h5>

<ul>
  <li>기존 Faster RCNN 중 RPN의 경우, Classification &amp; Bounding box Regression을 진행함</li>
  <li>이를 object detection에 바로 활용</li>
  <li>각 grid 별로 형성된 anchor box들에 대해 개별적으로 object 유형에 따른 softmax 및 좌표값을 계산</li>
  <li>Conv 결과값은 object class 별 probability + background + offset 4 points 값을 갖게 됨.</li>
</ul>

<p><img src="/assets/images/CV/SSD-3.png" alt="SSD-3" />
<br /><br /></p>

<hr />

<p><span style="color:red"> Doubt </span></p>

<p>자 IoU = Ground Truth와 bounding box와의 겹치는 영역의 비를 나타낸건데
지금 SSD에서 NMS를 적용하는 부분은 각 feature map에서 확인된 anchor box들에 대해서 적용하는 거임
따라서 NMS는 다수의 anchor box vs Ground truth 간 confidence score 및 IoU를 이용해서 하는건데</p>

<blockquote>
  <p>feature map에서 확인된 anchor box는 resizing이 된건데  ground truth와 어떻게 비교하지?</p>
</blockquote>

<p>matching strategy</p>

<p>사실 앞에서 만든 feature map 별 anchor box가 너무 많기 때문에 NMS를 통해서 수를 극적으로 줄여서 inference time을 개선</p>

<p>match 전략 : bounding box와 anchor box와의 IoU보는거래
그럼 결국 아래의 두가지 경우를 모두 보는 거 아닌가? 처음에 말했던 컨셉이 충돌되는 거 같은데</p>

<ol>
  <li>feature map anchor box vs ground truth IoU</li>
  <li>feature map anchor box vs bounding box IoU</li>
</ol>

<hr />

<p><br /></p>

<p>feature map에 anchor box를 적용했을 때의 문제점은 크기가 작은 object를 detect했을 때의 성능이 떨어짐</p>

<blockquote>
  <p>이에 대한 성능 보정을 위해 Data Augmentation을 진행</p>
</blockquote>

<p><br /></p>

<h4 id="data-augmentation">Data Augmentation</h4>

<ul>
  <li>학습 데이터셋의 규모를 키우는 방법</li>
  <li>원본에 추가되는 개념이라 성능이 떨어지지 않음</li>
  <li>단기간에 성능을 올리고 싶은 경우 적용</li>
  <li>데이터가 많아진다는 것은 overfitting을 줄일 수 있다는 것을 의미</li>
  <li>대부분 학습에 사용된 데이터의 품질은 좋지만 실제 테스트에 사용될 데이터들은 노이즈 등이 있어서 예측이 어려울 수 있음. 즉, 모델이 실제 환경에서도 잘 동작하도록 도와줌.</li>
</ul>

<p><br /></p>

<ol>
  <li>Mirroring</li>
  <li>Random Cropping</li>
  <li>Rotating</li>
  <li>Color shifting</li>
  <li>Shearing</li>
  <li>etc …</li>
</ol>]]></content><author><name>Written by Aaron</name></author><category term="ComputerVision" /><category term="ComputerVision" /><category term="SSD" /><summary type="html"><![CDATA[SSD(Single Shot multibox Detector)]]></summary></entry><entry><title type="html">Object Detection과 Segmentation</title><link href="/computervision/2022/04/30/Chapter1.html" rel="alternate" type="text/html" title="Object Detection과 Segmentation" /><published>2022-04-30T00:00:00+09:00</published><updated>2022-04-30T00:00:00+09:00</updated><id>/computervision/2022/04/30/Chapter1</id><content type="html" xml:base="/computervision/2022/04/30/Chapter1.html"><![CDATA[<h2 id="0-dependancy">0. Dependancy</h2>

<ul>
  <li>CNN</li>
  <li>feature extractor network(convolution, kernel(filter), pooling, feature map)</li>
  <li>VGG, rasnet etc</li>
</ul>

<hr />

<ul>
  <li>2012년도에 AlexNet이 ImageNet compatition에서 DL기반의 CNN을 사용</li>
  <li>이 시점을 기준으로 DL에 대한 performance 등 여러 가능성이 증가함</li>
</ul>

<p><br /><br /></p>

<h2 id="1-definition">1. Definition</h2>

<p><br /></p>

<h4 id="classification">Classification</h4>

<ul>
  <li>feature map과 label 등의 정보를 이용하여 이미지 분류</li>
</ul>

<h4 id="localization">Localization</h4>

<ul>
  <li>이미지 안에서 특정 영역을 한정 짓는 것</li>
  <li>하나의 이미지에 하나의 object를 bounding box로 지정하여 예측</li>
</ul>

<h4 id="detection">Detection</h4>

<ul>
  <li>하나의 이미지에 다수의 Object</li>
  <li>각 objectfmf bounding box로 구분하여 예측</li>
  <li>Region proposal과 classification
    <ul>
      <li>동시에 진행되면 1stage detector</li>
      <li>region proposal &gt; classification이 순차적으로 진행되면 2stage detector</li>
      <li>포인트는 성능과 수행 시간이 반비례</li>
    </ul>
  </li>
</ul>

<h4 id="segmentation">Segmentation</h4>

<ul>
  <li>Detection에서 bounding box단위가 아니라 pixel 단위로 구분</li>
</ul>

<p><br /><br /></p>

<h2 id="2-overview-of-object-detection">2. Overview of Object Detection</h2>

<ul>
  <li>Region proposal</li>
  <li>DL network
    <ul>
      <li>Feature extractor network</li>
      <li>FPN
        <ul>
          <li>feature extractor와 prediction network를 연결</li>
          <li>작은 object들에 대한 정보를 체계화시켜 분류</li>
          <li>Object가 크다는 전제조건이 있다면 무시할 수도 있을지도..</li>
        </ul>
      </li>
      <li>Object Detection network</li>
    </ul>
  </li>
  <li>기타 요소
    <ul>
      <li><strong>IoU</strong></li>
      <li><strong>NMS</strong></li>
      <li><strong>mAP</strong></li>
      <li>Anchor Box</li>
      <li>etc …</li>
    </ul>
  </li>
  <li>Issue
    <ul>
      <li>Classification + Regression을 동시에 진행</li>
      <li>다양한 크기의 object</li>
      <li>Detection time</li>
      <li>명확하지 않은 이미지</li>
      <li>train data set 부족(Annotation)</li>
    </ul>
  </li>
</ul>

<p><br /><br /></p>

<h2 id="3-overview-of-object-localization">3. Overview of Object Localization</h2>

<ul>
  <li>단일 이미지의 localization의 경우 비교적 쉬움
    <ul>
      <li>classification의 flow와 동일한데, bounding box에 대한 regression이 추가됨</li>
      <li>bounding box에 대한 annotation file이 별도로 필요</li>
      <li><strong>좌표값을 나타내는 annotation file의 경우 데이터셋 및 알고리즘에 따라 차이가 있을 수 있음.</strong></li>
    </ul>
  </li>
  <li>결과는 예측 class에 따른 confidence(class) score와 bounding box의 좌표값을 출력</li>
</ul>

<p><img src="/assets/images/CV/ObjectDetection-1.png" alt="ObjectDetection-1" /></p>
<blockquote>
  <p>softmax와 같은 분류기를 FC layer에 포함시켜서 설명하는 경우도 있음</p>
</blockquote>

<p><br /></p>

<ul>
  <li>보통 feature map에서 말하는 채널의 경우 이미지를 표현할 때 RGB에 의해 3개의 채널로 표현</li>
  <li>근데,, 경우에 따라 채널 수가 매우 많아지는 경우가 있는 거 같은데;;</li>
</ul>

<p><br /><br /></p>

<h2 id="4-overview-of-object-deteciton">4. Overview of Object Deteciton</h2>

<ul>
  <li>2개 이상의 object를 검출</li>
  <li>한 이미지에 비슷한 feature인 object들이 있을 때 bounding box regression에서 오류가 많이 발생</li>
  <li>Region Proposal
    <ul>
      <li>Sliding Window</li>
      <li>Selective Search(SS)</li>
    </ul>
  </li>
</ul>

<h4 id="sliding-window">Sliding Window</h4>

<p><img src="/assets/images/CV/ObjectDetection-2.png" alt="ObjectDetection-2" /></p>

<ol>
  <li>다양한 크기의 window를 sliding하는 방식</li>
  <li>window size를 고정하고 이미지 scale을 변화시킨 여러 이미지를 사용하는 방식
    <ul>
      <li>object detection의 초기 기법</li>
      <li>object가 없더라도 무조건 모든 영역을 sliding, 수행 시간이 오래걸리고 성능이 낮음</li>
      <li>window 하나에 하나의 object를 detection하는 것이 주 목적이였으나, 경우에 따라 또다시 여러 object가 위치할 수도 있음.</li>
    </ul>
  </li>
</ol>

<p><br /></p>

<h4 id="selective-search">Selective Search</h4>

<p>x</p>
<ul>
  <li>object가 있을 법한 영역(bounding box) 후보리스트를 작성해서 최종 후보를 도출</li>
  <li>빠른 detection과 높은 recall 예측 성능을 동시에 만족
    <ul>
      <li>flow에 따라 네트워크 안에 포함하여 구성할 수도 별도로 관리할 수도 있음</li>
    </ul>
  </li>
  <li>최초의 pixel단위 intensity 기반, graph-based segmentation &gt; over segmentation</li>
  <li>이미지의 컬러나 무늬, 크기에 따라 유사한 영역을 반복적으로 grouping 하는 region proposal</li>
</ul>

<p><img src="/assets/images/CV/ObjectDetection-3.png" alt="ObjectDetection-3" /></p>

<p><br /></p>

<h4 id="iouintersection-over-union">IoU(Intersection over Union)</h4>

<ul>
  <li>실제 object annotated bounding box의 영역과 예측한 bounding box 간 겹치는 부분의 비율이 얼마나 되는지를 통해 잘 예측했는지를 평가 (ratio 0 ~ 1)</li>
  <li>compatition에 따른 IoU 활용
    <ul>
      <li>PASCAL VOC : IoU 0.5 기준 성능 평가</li>
      <li>MS-COCO : IoU를 0.5 ~ 0.95범위에서 다양하게 성능 평가, 2차원 metric으로 성능 평가할 것으로 추정</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/images/CV/ObjectDetection-4.png" alt="ObjectDetection-4" width="50%" class="center" /></p>

<p><br /></p>

<h4 id="nmsnon-max-suppression">NMS(Non Max Suppression)</h4>

<ul>
  <li>object 주변에 있는 bounding box중에서 가장 확실한 box를 제외한 나머지를 제외 시켜주는 기법</li>
  <li>object detection의 정확도를 올리기 위해 사전에 false positive인 bounding box까지 모두 추천한 후, 그 중 가장 확실한 box를 선택해주는 기법이 필요</li>
  <li>input
    <ul>
      <li>Confidence score</li>
      <li>IoU score</li>
      <li>Confidence score가 높을수록, IoU가 낮을수록 더 많은 box가 제거됨.</li>
    </ul>
  </li>
</ul>

<p><br /></p>

<h4 id="object-detection-성능-평가-metrics---mapmean-average-precision--auc">Object detection 성능 평가 metrics - mAP(mean Average precision) =~ AUC</h4>

<ul>
  <li>Recall 변화에 따른 Precision 값을 평균한 성능 수치
    <ol>
      <li>IoU &amp; Confidence score</li>
      <li>Confusion metrics</li>
    </ol>
  </li>
</ul>

<p><img src="/assets/images/CV/ObjectDetection-5.png" alt="ObjectDetection-5" width="70%" /></p>

<blockquote>
  <p>예측을 했냐? P(positive) 안했냐? N(negative), 그 예측 결과가 실제값과 일치하냐? T(true) 일치하지 않느냐? F(false)</p>
</blockquote>

<p><img src="/assets/images/CV/ObjectDetection-6.png" alt="ObjectDetection-6" /></p>

<ol>
  <li>Precision-Recall curve(binary classification에서 잘 활용)</li>
</ol>

<ul>
  <li>Precision(정밀도) = $TP/(FP+TP)$</li>
  <li>Recall(재현율) = $TP/(FN+TP)$</li>
</ul>

<p><img src="/assets/images/CV/ObjectDetection-7.png" alt="ObjectDetection-7" width="50%" /></p>

<p><img src="/assets/images/CV/ObjectDetection-9.png" alt="ObjectDetection-9" width="50%" /></p>

<blockquote>
  <p>일반적으로 Precision-Recall curve는 지그재그 형태로 나타남</p>
</blockquote>

<h4 id="precision--recall-trade-off">Precision &amp; Recall Trade-off</h4>
<ul>
  <li>=~ Sensitivity &amp; Specificity</li>
</ul>

<p><img src="/assets/images/CV/ObjectDetection-8.png" alt="ObjectDetection-8" /></p>

<p><br /></p>

<p>AP(Average Precision)</p>

<ul>
  <li>하나의 object에 대한 성능 수치를 평가하는 지표</li>
  <li>굳이 적용한다면 localization에 적용하는 것이 맞는 표현</li>
</ul>

<p>mAP(mean Average Precision)</p>

<ul>
  <li>object detection은 다수의 Object가 존재하기 때문에, 각 object의 AP를 계산 후, mean 계산</li>
</ul>

<p><br /><br /></p>

<h2 id="5-object-detection--segmentation을-위한-주요-데이터-셋">5. Object Detection &amp; Segmentation을 위한 주요 데이터 셋</h2>

<ol>
  <li><strong>PASCAL VOC</strong></li>
</ol>

<ul>
  <li>object 유형이 20개 정도</li>
  <li>annotation : xml file</li>
</ul>

<ol>
  <li><strong>MS-COCO</strong></li>
</ol>

<ul>
  <li>object 유형이 80개 정도</li>
  <li>pretrained model 배표</li>
  <li>annotation : json format</li>
</ul>

<ol>
  <li><strong>Google Open Image</strong></li>
</ol>

<ul>
  <li>object 유형이 600개 정도</li>
  <li>annotation : csv format</li>
</ul>

<p><br /></p>

<h4 id="pascal-voc">PASCAL VOC</h4>

<ul>
  <li>Annotation : Detection 정보들을 설명 파일로 제공하는 것, 연결되는 이미지명을 포함한 각 object별 bounding box 좌표 정보<span style="color:red">(좌상단, 우하단)</span>가 수록되어있음.</li>
  <li>ImageSet : train, validation set</li>
  <li>JPEGImages : Detection &amp; Segmentation에 사용될 원본 이미지</li>
  <li>SegmentationClass : Semantic Segmentation에 사용될 이미지(별도의 Object 구분이 없음)</li>
  <li>SegmentationObject : Instance Segmentation에 사용될 이미지(object가 존재)</li>
</ul>

<p><img src="/assets/images/CV/ObjectDetection-10.png" alt="ObjectDetection-10" width="26%" /> <img src="/assets/images/CV/ObjectDetection-11.png" alt="ObjectDetection-11" width="30%" /></p>

<p><a href="https://velog.io/@cha-suyeon/%EB%94%A5%EB%9F%AC%EB%8B%9D-Segmentation1-%EA%B0%9C%EB%85%90-%EC%9A%A9%EC%96%B4-%EC%A2%85%EB%A5%98Semantic-Instance-segmentation">Figure reference site</a></p>

<p><br /></p>

<h4 id="ms-coco">MS-COCO</h4>

<ul>
  <li>tensorflow api 등 open source 패키지들은 MS-COCO를 이용한 pretrained model 제공</li>
  <li>ID가 부여되었지만 데이터셋이 없는 것이 존재</li>
  <li>bounding box의 경우 <span style="color:red">좌상단 좌표와 width, height</span>가 표기</li>
  <li>annotation file은 json file 하나로 구성되어있는데 one line 처리가 되어있음..</li>
</ul>

<blockquote>
  <p>json format의 경우 별도의 주석 기능을 지원하지 않음. 굳이 넣는다면 절대 사용하지 않을 키 값을 만들어서 지정해주기.</p>
</blockquote>

<ul>
  <li>이미지 하나에 여러 object들을 가지고 있어서 타 데이터 셋보다 난이도가 높은 데이터 제공한다는 장점이 있음.</li>
</ul>

<p><br /><br /></p>

<h2 id="6-overview-of-opencv">6. Overview of OpenCV</h2>

<h4 id="python-기반-주요-이미지-라이브러리">Python 기반 주요 이미지 라이브러리</h4>

<ul>
  <li>PIL</li>
  <li>처리 성능이 상대적으로 느림</li>
  <li>Scikit Image</li>
  <li>Scipy 확장, numpy 기반</li>
  <li>전반적인 컴퓨터 비전 기능 제공</li>
  <li>OpenCN</li>
  <li>open source 기반 최고의 인기 라이브러리</li>
  <li>컴퓨터 비전 기능 일반화에 기여</li>
  <li>가장 중요한 부분 : <span style="color:red">image load(imread)할 때 RGB기반이 아니라 BGR기반으로 메모리에 load</span></li>
</ul>

<p><br /></p>

<p><img src="/assets/images/CV/CherryBlossom.jpeg" alt="RGB" width="60%" /></p>

<p><br /></p>

<p><img src="/assets/images/CV/CherryBlossom-BGR.png" alt="BGR" width="70%" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="c1">#!/usr/bin/env python
</span>
<span class="n">img_array</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="n">imread</span><span class="p">(</span><span class="s">'figs/CherryBlossom.jpeg'</span><span class="p">)</span>
<span class="c1">#plt.imshow(img_array)
</span></code></pre></div></div>

<h4 id="image-resolution-fps-detection-correlation">Image resolution, FPS, detection correlation</h4>

<ul>
  <li>Resolution이 높으면 detection 성능은 좋으나 FPS가 낮음</li>
  <li>알고리즘 개선이 없다면 진보된 하드웨어가 필요…</li>
</ul>

<p><br /><br /></p>]]></content><author><name>Written by Aaron</name></author><category term="ComputerVision" /><category term="ComputerVision" /><category term="ObjectDetection" /><summary type="html"><![CDATA[0. Dependancy]]></summary></entry><entry><title type="html">RCNN, Fast-RCNN, Faster-RCNN</title><link href="/computervision/2022/04/30/Chapter2.html" rel="alternate" type="text/html" title="RCNN, Fast-RCNN, Faster-RCNN" /><published>2022-04-30T00:00:00+09:00</published><updated>2022-04-30T00:00:00+09:00</updated><id>/computervision/2022/04/30/Chapter2</id><content type="html" xml:base="/computervision/2022/04/30/Chapter2.html"><![CDATA[<hr />

<h2 id="1-rcnnregions-with-cnn">1. RCNN(Regions with CNN)</h2>

<ul>
  <li>CNN 공부하다보면 어떤 곳에서는 Region Proposal을 network 안에 포함하여 설명하는 글이 있음</li>
  <li>CNN에 Region Proposal을 넣은 것이 RCNN</li>
</ul>

<p><br /></p>

<p><img src="/assets/images/CV/RCNN-1.png" alt="RCNN-1" /></p>

<p><br /><br /></p>

<ol>
  <li>단일 이미지에 대해 Selective Search를 이용하여 2000개의 Region proposal</li>
  <li>Selective Search를 통해 나온 이미지들을 AlexNet에 넣기 위해서는 227*227 image crop</li>
  <li>마지막에 classification을 위해 사용했던 softmax 대신 SVM를 채택하여 성능이 향상됨.
    <ul>
      <li>SVM(Support Vector Machine) : 특정 함수를 만족하는 regression line은 무수히 많지만 그 중에서 잔차의 합이 최소가 되는 line을 찾는 방법</li>
      <li>성능은 향상됐으나, 약 2000개의 crop image를 processing하기 때문에 시간이 오래 걸림</li>
      <li>object detection 및 region proposal 성능 입증</li>
    </ul>
  </li>
</ol>

<p><br /></p>

<h4 id="rcnn-training">RCNN Training</h4>

<p><br /></p>

<p><img src="/assets/images/CV/RCNN-2.png" alt="RCNN-2" /></p>

<ol>
  <li>데이터 셋 상 Annotation file기반 Ground truth와 Selective Search를 통한 predicted bounding box 확인</li>
  <li>1000개의 class로 되어있는 ImageNet으로 Feature Extractor pretraining</li>
  <li>pretrained model에 사용할 데이터 셋을 fine-tuning
    <ul>
      <li>Gronud truth와 Selective Search bounding box 간 IoU가 특정 threshold(PASCAL 데이터 셋일 때 0.5일 것으로 보임)이상인 것들만 학습을 시키고 나머지는 background로 find-tuning</li>
    </ul>
  </li>
  <li>만들어진 Feature Map을 이용한 1D FC input이 만들어짐</li>
  <li>SVM을 이용한 Ground truth만을 기준으로 학습하되 IoU가 0.3 이하는 background로 설정, <span style="color:red">0.3이상인데 GT가 아닌 것은 무시???</span></li>
</ol>

<p><br /></p>

<h4 id="bounding-box-regression">Bounding Box Regression</h4>

<p><br /></p>

<ul>
  <li>bounding box와 Ground truth의 각 중심점 간 거리가 최소가 되도록 학습</li>
  <li>우리가 원하는 건 bounding box 변수 P에 대한 tranformation(기존의 vector(여기서는 bounding box P)를 다른 좌표 공간으로 정의하여 달리 표현(여기서는 G_hat)) 함수</li>
  <li>transformation 함수는 CNN layter 중 weight vector가 가미된 손실함수.</li>
  <li>손실 함수 상에서 사용된 가중치를 학습시키기 위해 ridge regression을 사용하여 regression target를 정의</li>
  <li>전반적인 bounding box regression의 경우 RCNN상 Selective Search로 찾은 약 2K image에 대해 모두 진행하지 않고 특정 IoU threshold 이상은 값을 가지는 것에 대해서만 진행</li>
</ul>

<p><img src="/assets/images/CV/RCNN-3.png" alt="RCNN-3" width="60%" /></p>

<p><br /><br /></p>

<h2 id="2-sppspatial-pyramid-pooling-net">2. SPP(Spatial Pyramid Pooling) Net</h2>

<ul>
  <li>RCNN의 문제점
    <ul>
      <li><span style="color:red">detection 시간이 너무 오래 걸림</span></li>
      <li>image crop &amp; warp로 인해 원래의 이미지가 아닌 다른 이미지로 인식이 됨.</li>
    </ul>
  </li>
</ul>

<p><br /></p>

<h4 id="rcnn-개선-방안">RCNN 개선 방안</h4>

<ul>
  <li>원본 이미지에 대해 feature map을 만든 후, bounding box에 대한 부분을 feature map에 mapping하면 해결 될 것으로 보였으나, dense layer 1D FC input size가 고정되지 않아 진행할 수 없었음.</li>
</ul>

<p><br /></p>

<p><img src="/assets/images/CV/RCNN-4.png" alt="RCNN-4" /></p>

<ul>
  <li>이에 대해 SPP layer를 추가하여 서로 다른 크기를 가진 이미지를 vector로 변환하여 dense layer인 1D FC input을 제공하게끔 함.</li>
  <li>SPP는 CNN image classification에서 서로 다른 이미지의 크기를 고정된 크기로 변환하는 기법으로 소개, 이는 Spatial Pyramid Matching 기법에 근단을 둠</li>
  <li><strong>SPM(Spatial Pyramid Matching)</strong> : 이미지를 grid layout(위치)을 기반으로 하여 각 feature들이 위치 별로 얼마나 분포하고 있는 지를 파악한 후, 각 위치 별 가장 빈번한 feature의 특성이 반영된 새로운 속성들이 생김. 새롭게 확인된 속성들을 이용하여 새로운 분류를 할 수 있게 됨. <span style="color:blue">결국 파생 변수…아이디어 싸움</span></li>
</ul>

<p><br /></p>

<p><img src="/assets/images/CV/RCNN-5.png" alt="RCNN-5" /></p>

<ul>
  <li>결국 SPP는 위의 SPM에 max pooling만 추가 적용한 것임.</li>
  <li>여기서 질문.. 서로 다른 feature map size임에도 불구하고 SPP를 적용하여 문제를 해결할 수 있으나, 2n x 2n의 feature들이여야 1,4,12분면으로 만들 수 있는 거 아니니?</li>
</ul>

<p><br /></p>

<p><img src="/assets/images/CV/RCNN-6.png" alt="RCNN-6" /></p>

<p><br /><br /></p>

<h4 id="rcnn의-발전">RCNN의 발전</h4>

<ul>
  <li>R-CNN : 한 image에 대해 2천개의 bounding box를 모두 학습하여 별도의 SVM을 통해 classification</li>
  <li>SPP layer 도입</li>
  <li>Fast R-CNN : 여전히 Selective Search포함되어 있으며 spp layer를 ROI pooling으로 대체. 또한 SVM이 삭제되고 다시 softmax를 도입. End-To-End Network learning(ROI는 제외), Multi-task loss 함수로 classification과 regression를 함께 최적화(loss 줄이기)</li>
  <li>
    <p>Fater R-CNN : RPN이 추가됨.</p>
  </li>
  <li>ROI : 방법론이 어떻든 region proposal로 제시된 영역을 일컫는 것으로 보임.</li>
</ul>

<p><br /></p>

<h4 id="fast-r-cnn">Fast R-CNN</h4>

<p><br /></p>

<p><img src="/assets/images/CV/RCNN-7.png" alt="RCNN-7" /></p>

<ol>
  <li>원본 이미지에 대해 한 번만 학습하여 feature map을 만듬</li>
  <li>Selective Search로 찾은 bounding box에 대하여 이미 만들어놓은 feature map에 mapping</li>
  <li>ROI logic을 적용하여 resizing, max pooling을 적용하여 크기에 상관없이 정해진 feature extract</li>
  <li>FC layer &gt; softmax를 통해 분류</li>
</ol>

<p><br /></p>

<h4 id="faster-r-cnn">Faster R-CNN</h4>

<ul>
  <li>RNN + Fast R-CNN</li>
  <li>object detection을 구성하는 모든 요소들을 모두 딥러닝만으로 구성한 첫 알고리즘</li>
  <li>Selective Search를 통한 ROI logic을 RPM으로 대체</li>
  <li>GPU 사용으로 빠른 학습</li>
  <li>end-to-end Network 학습
    <ul>
      <li>다른 네트워크 개입없이 하나의 네트워크만으로 전체 파이프라인 네트워크를 이루는 것으로 보임.</li>
      <li>병렬 처리가 안된다는 안점?이 존재할 거 같다.</li>
      <li>분석이 리소스에 의존적이다. 메모리 제한</li>
    </ul>
  </li>
</ul>

<p><br /></p>

<p><img src="/assets/images/CV/RCNN-8.png" alt="RCNN-8" /></p>

<p><br /><br /></p>

<h4 id="anchor-box">Anchor Box</h4>

<ul>
  <li>Selective Search를 대체하여 end-to-end network의 기반이 된 기술</li>
  <li>형성된 feature map을 기준으로 각 grid(value …)를 기준으로 다음의 특징을 가지는 anchor box를 작성
    <ul>
      <li>1배수, 2배수, 3배수 // 1:1, 1:2, 2:1으로 총 9개의 anchor box 적용하는 것이 일반적</li>
      <li>근데 yolo v2는 coco 데이터 셋의 bounding box에 k-means clustering을 적용해보니 anchor box를 5개로 설정하는 것이 precision과 recall측면에서 좋은 결과를 낸다고 결론지음.</li>
    </ul>
  </li>
</ul>

<p><br /></p>

<p><img src="/assets/images/CV/RCNN-9.png" alt="RCNN-9" /></p>

<p><br /><br /></p>

<h4 id="rpn-network">RPN Network</h4>

<ol>
  <li>원본 이미지를 이용하여 feature map(n x m x c)을 얻는다.</li>
  <li>feature map에 대하여 3x3 conv 연산을 적용한다. 크기에 변화가 없도록 padding을 추가한다.</li>
  <li>1x1conv을 통하여 channel이 축소화 된 2가지의 feature map을 출력하는데..
    <ul>
      <li>
        <dl>
          <dt>이진 분류(object가 있나? 없나?)를 위한 feature map 출력</dt>
          <dd>1x1conv을 이용한 9(anchor boxs) channel feature map</dd>
        </dl>
      </li>
      <li>bounding box(x, y, w, h)를 위해 1x1conv를 이용한 4(bb coordinate)x9(anchor box) channel feature map</li>
    </ul>
  </li>
</ol>

<blockquote>
  <p>근데 RPN에 대해 다른 자료들을 찾아보면 이진 분류를 위한 feature map은 9개가 아니라 9x2 channel의 feature map을 출력한다고 기입되어있음. 이 때의 2는 object가 있냐 없냐에 대한 binary class의 수이고, bounding box coordinate에 해당하는 좌표 정보의 갯수의 컨셉과 동일한 것으로 추정. 설마 sigmoid로 하면 9 channel로 구성해도 알아서 2배수로 해주는건가…</p>
</blockquote>

<h4 id="sigmoid-vs-softmax">Sigmoid vs Softmax</h4>

<p><br /></p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">N</th>
      <th style="text-align: center">Sigmoid</th>
      <th style="text-align: center">softmax</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">1</td>
      <td style="text-align: center">binary-classification</td>
      <td style="text-align: center">multi-classification</td>
    </tr>
    <tr>
      <td style="text-align: center">2</td>
      <td style="text-align: center">sum of P != 1</td>
      <td style="text-align: center">sum of P == 1</td>
    </tr>
    <tr>
      <td style="text-align: center">3</td>
      <td style="text-align: center">output is not real P</td>
      <td style="text-align: center">real P</td>
    </tr>
  </tbody>
</table>

<p><br /></p>

<h4 id="rpn-network-configuration">RPN Network configuration</h4>

<p><img src="/assets/images/CV/RCNN-10.png" alt="RCNN-10" /></p>

<p><br /></p>

<h4 id="positive-negative-anchor-box">Positive, Negative anchor box</h4>

<ul>
  <li>모든 anchor box에 대해 RPN bounding box reg를 적용하지 않는다. 왜? 너무 많으니까</li>
  <li>ground truth와 anchor box간 IoU를 이용하여 positive와 negative로 구분함.
    <ul>
      <li>IoU가 0.7 이상이거나 가장 높은 anchor는 <span style="color:blue">positive</span></li>
      <li>IoU가 0.3보다 낮으면 <span style="color:red">Negative</span></li>
    </ul>
  </li>
</ul>

<p><br /></p>

<h4 id="bounding-box-regression-1">bounding box regression</h4>

<p><img src="/assets/images/CV/RCNN-11.png" alt="RCNN-11" /></p>

<p><br /><br /></p>

<h4 id="rpn-손실함수">RPN 손실함수</h4>

<p><img src="/assets/images/CV/RCNN-12.png" alt="RCNN-12" /></p>

<h4 id="faster-r-cnn-training">Faster R-CNN training</h4>

<p><br /></p>

<ol>
  <li>RPN을 선행 학습</li>
  <li>Fast RCNN Classification / Regression 학습</li>
  <li>2의 loss를 이용해 RPN을 fine tuning</li>
  <li>전반적인 process를 진행</li>
</ol>

<p><br /><br /></p>

<h2 id="rcnn-summary">RCNN Summary</h2>

<p><br /></p>

<p><img src="/assets/images/CV/RCNN-13.png" alt="RCNN-13" /></p>

<p><br /></p>

<ul>
  <li>RCNN : Selective Search로 인한 2000개의 conv를 적용하여 시간이 오래걸리는 단점</li>
  <li>Fast RCNN : RCNN의 문제점을 해결하기 위해 ROI를 적용하고 SVM 제거</li>
  <li>Faster RCNN : Selective Search를 제거하고 RPN을 통해 딥러닝으로만 구성된 네트워크</li>
</ul>

<p><br /></p>

<hr />

<p><br /></p>]]></content><author><name>Written by Aaron</name></author><category term="ComputerVision" /><category term="ComputerVision" /><category term="RCNN" /><summary type="html"><![CDATA[]]></summary></entry></feed>