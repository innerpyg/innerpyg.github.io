<h2 id="0-dependancy">0. Dependancy</h2>

<ul>
  <li>CNN</li>
  <li>feature extractor network(convolution, kernel(filter), pooling, feature map)</li>
  <li>VGG, rasnet etc</li>
</ul>

<hr />

<ul>
  <li>2012년도에 AlexNet이 ImageNet compatition에서 DL기반의 CNN을 사용</li>
  <li>이 시점을 기준으로 DL에 대한 performance 등 여러 가능성이 증가함</li>
</ul>

<p><br /><br /></p>

<h2 id="1-definition">1. Definition</h2>

<p><br /></p>

<h4 id="classification">Classification</h4>

<ul>
  <li>feature map과 label 등의 정보를 이용하여 이미지 분류</li>
</ul>

<h4 id="localization">Localization</h4>

<ul>
  <li>이미지 안에서 특정 영역을 한정 짓는 것</li>
  <li>하나의 이미지에 하나의 object를 bounding box로 지정하여 예측</li>
</ul>

<h4 id="detection">Detection</h4>

<ul>
  <li>하나의 이미지에 다수의 Object</li>
  <li>각 objectfmf bounding box로 구분하여 예측</li>
  <li>Region proposal과 classification
    <ul>
      <li>동시에 진행되면 1stage detector</li>
      <li>region proposal &gt; classification이 순차적으로 진행되면 2stage detector</li>
      <li>포인트는 성능과 수행 시간이 반비례</li>
    </ul>
  </li>
</ul>

<h4 id="segmentation">Segmentation</h4>

<ul>
  <li>Detection에서 bounding box단위가 아니라 pixel 단위로 구분</li>
</ul>

<p><br /><br /></p>

<h2 id="2-overview-of-object-detection">2. Overview of Object Detection</h2>

<ul>
  <li>Region proposal</li>
  <li>DL network
    <ul>
      <li>Feature extractor network</li>
      <li>FPN
        <ul>
          <li>feature extractor와 prediction network를 연결</li>
          <li>작은 object들에 대한 정보를 체계화시켜 분류</li>
          <li>Object가 크다는 전제조건이 있다면 무시할 수도 있을지도..</li>
        </ul>
      </li>
      <li>Object Detection network</li>
    </ul>
  </li>
  <li>기타 요소
    <ul>
      <li><strong>IoU</strong></li>
      <li><strong>NMS</strong></li>
      <li><strong>mAP</strong></li>
      <li>Anchor Box</li>
      <li>etc …</li>
    </ul>
  </li>
  <li>Issue
    <ul>
      <li>Classification + Regression을 동시에 진행</li>
      <li>다양한 크기의 object</li>
      <li>Detection time</li>
      <li>명확하지 않은 이미지</li>
      <li>train data set 부족(Annotation)</li>
    </ul>
  </li>
</ul>

<p><br /><br /></p>

<h2 id="3-overview-of-object-localization">3. Overview of Object Localization</h2>

<ul>
  <li>단일 이미지의 localization의 경우 비교적 쉬움
    <ul>
      <li>classification의 flow와 동일한데, bounding box에 대한 regression이 추가됨</li>
      <li>bounding box에 대한 annotation file이 별도로 필요</li>
      <li><strong>좌표값을 나타내는 annotation file의 경우 데이터셋 및 알고리즘에 따라 차이가 있을 수 있음.</strong></li>
    </ul>
  </li>
  <li>결과는 예측 class에 따른 confidence(class) score와 bounding box의 좌표값을 출력</li>
</ul>

<p><img src="/assets/images/CV/ObjectDetection-1.png" alt="ObjectDetection-1" /></p>
<blockquote>
  <p>softmax와 같은 분류기를 FC layer에 포함시켜서 설명하는 경우도 있음</p>
</blockquote>

<p><br /></p>

<ul>
  <li>보통 feature map에서 말하는 채널의 경우 이미지를 표현할 때 RGB에 의해 3개의 채널로 표현</li>
  <li>근데,, 경우에 따라 채널 수가 매우 많아지는 경우가 있는 거 같은데;;</li>
</ul>

<p><br /><br /></p>

<h2 id="4-overview-of-object-deteciton">4. Overview of Object Deteciton</h2>

<ul>
  <li>2개 이상의 object를 검출</li>
  <li>한 이미지에 비슷한 feature인 object들이 있을 때 bounding box regression에서 오류가 많이 발생</li>
  <li>Region Proposal
    <ul>
      <li>Sliding Window</li>
      <li>Selective Search(SS)</li>
    </ul>
  </li>
</ul>

<h4 id="sliding-window">Sliding Window</h4>

<p><img src="/assets/images/CV/ObjectDetection-2.png" alt="ObjectDetection-2" /></p>

<ol>
  <li>다양한 크기의 window를 sliding하는 방식</li>
  <li>window size를 고정하고 이미지 scale을 변화시킨 여러 이미지를 사용하는 방식
    <ul>
      <li>object detection의 초기 기법</li>
      <li>object가 없더라도 무조건 모든 영역을 sliding, 수행 시간이 오래걸리고 성능이 낮음</li>
      <li>window 하나에 하나의 object를 detection하는 것이 주 목적이였으나, 경우에 따라 또다시 여러 object가 위치할 수도 있음.</li>
    </ul>
  </li>
</ol>

<p><br /></p>

<h4 id="selective-search">Selective Search</h4>

<p>x</p>
<ul>
  <li>object가 있을 법한 영역(bounding box) 후보리스트를 작성해서 최종 후보를 도출</li>
  <li>빠른 detection과 높은 recall 예측 성능을 동시에 만족
    <ul>
      <li>flow에 따라 네트워크 안에 포함하여 구성할 수도 별도로 관리할 수도 있음</li>
    </ul>
  </li>
  <li>최초의 pixel단위 intensity 기반, graph-based segmentation &gt; over segmentation</li>
  <li>이미지의 컬러나 무늬, 크기에 따라 유사한 영역을 반복적으로 grouping 하는 region proposal</li>
</ul>

<p><img src="/assets/images/CV/ObjectDetection-3.png" alt="ObjectDetection-3" /></p>

<p><br /></p>

<h4 id="iouintersection-over-union">IoU(Intersection over Union)</h4>

<ul>
  <li>실제 object annotated bounding box의 영역과 예측한 bounding box 간 겹치는 부분의 비율이 얼마나 되는지를 통해 잘 예측했는지를 평가 (ratio 0 ~ 1)</li>
  <li>compatition에 따른 IoU 활용
    <ul>
      <li>PASCAL VOC : IoU 0.5 기준 성능 평가</li>
      <li>MS-COCO : IoU를 0.5 ~ 0.95범위에서 다양하게 성능 평가, 2차원 metric으로 성능 평가할 것으로 추정</li>
    </ul>
  </li>
</ul>

<p><img src="/assets/images/CV/ObjectDetection-4.png" alt="ObjectDetection-4" width="50%" class="center" /></p>

<p><br /></p>

<h4 id="nmsnon-max-suppression">NMS(Non Max Suppression)</h4>

<ul>
  <li>object 주변에 있는 bounding box중에서 가장 확실한 box를 제외한 나머지를 제외 시켜주는 기법</li>
  <li>object detection의 정확도를 올리기 위해 사전에 false positive인 bounding box까지 모두 추천한 후, 그 중 가장 확실한 box를 선택해주는 기법이 필요</li>
  <li>input
    <ul>
      <li>Confidence score</li>
      <li>IoU score</li>
      <li>Confidence score가 높을수록, IoU가 낮을수록 더 많은 box가 제거됨.</li>
    </ul>
  </li>
</ul>

<p><br /></p>

<h4 id="object-detection-성능-평가-metrics---mapmean-average-precision--auc">Object detection 성능 평가 metrics - mAP(mean Average precision) =~ AUC</h4>

<ul>
  <li>Recall 변화에 따른 Precision 값을 평균한 성능 수치
    <ol>
      <li>IoU &amp; Confidence score</li>
      <li>Confusion metrics</li>
    </ol>
  </li>
</ul>

<p><img src="/assets/images/CV/ObjectDetection-5.png" alt="ObjectDetection-5" width="70%" /></p>

<blockquote>
  <p>예측을 했냐? P(positive) 안했냐? N(negative), 그 예측 결과가 실제값과 일치하냐? T(true) 일치하지 않느냐? F(false)</p>
</blockquote>

<p><img src="/assets/images/CV/ObjectDetection-6.png" alt="ObjectDetection-6" /></p>

<ol>
  <li>Precision-Recall curve(binary classification에서 잘 활용)</li>
</ol>

<ul>
  <li>Precision(정밀도) = $TP/(FP+TP)$</li>
  <li>Recall(재현율) = $TP/(FN+TP)$</li>
</ul>

<p><img src="/assets/images/CV/ObjectDetection-7.png" alt="ObjectDetection-7" width="50%" /></p>

<p><img src="/assets/images/CV/ObjectDetection-9.png" alt="ObjectDetection-9" width="50%" /></p>

<blockquote>
  <p>일반적으로 Precision-Recall curve는 지그재그 형태로 나타남</p>
</blockquote>

<h4 id="precision--recall-trade-off">Precision &amp; Recall Trade-off</h4>
<ul>
  <li>=~ Sensitivity &amp; Specificity</li>
</ul>

<p><img src="/assets/images/CV/ObjectDetection-8.png" alt="ObjectDetection-8" /></p>

<p><br /></p>

<p>AP(Average Precision)</p>

<ul>
  <li>하나의 object에 대한 성능 수치를 평가하는 지표</li>
  <li>굳이 적용한다면 localization에 적용하는 것이 맞는 표현</li>
</ul>

<p>mAP(mean Average Precision)</p>

<ul>
  <li>object detection은 다수의 Object가 존재하기 때문에, 각 object의 AP를 계산 후, mean 계산</li>
</ul>

<p><br /><br /></p>

<h2 id="5-object-detection--segmentation을-위한-주요-데이터-셋">5. Object Detection &amp; Segmentation을 위한 주요 데이터 셋</h2>

<ol>
  <li><strong>PASCAL VOC</strong></li>
</ol>

<ul>
  <li>object 유형이 20개 정도</li>
  <li>annotation : xml file</li>
</ul>

<ol>
  <li><strong>MS-COCO</strong></li>
</ol>

<ul>
  <li>object 유형이 80개 정도</li>
  <li>pretrained model 배표</li>
  <li>annotation : json format</li>
</ul>

<ol>
  <li><strong>Google Open Image</strong></li>
</ol>

<ul>
  <li>object 유형이 600개 정도</li>
  <li>annotation : csv format</li>
</ul>

<p><br /></p>

<h4 id="pascal-voc">PASCAL VOC</h4>

<ul>
  <li>Annotation : Detection 정보들을 설명 파일로 제공하는 것, 연결되는 이미지명을 포함한 각 object별 bounding box 좌표 정보<span style="color:red">(좌상단, 우하단)</span>가 수록되어있음.</li>
  <li>ImageSet : train, validation set</li>
  <li>JPEGImages : Detection &amp; Segmentation에 사용될 원본 이미지</li>
  <li>SegmentationClass : Semantic Segmentation에 사용될 이미지(별도의 Object 구분이 없음)</li>
  <li>SegmentationObject : Instance Segmentation에 사용될 이미지(object가 존재)</li>
</ul>

<p><img src="/assets/images/CV/ObjectDetection-10.png" alt="ObjectDetection-10" width="26%" /> <img src="/assets/images/CV/ObjectDetection-11.png" alt="ObjectDetection-11" width="30%" /></p>

<p><a href="https://velog.io/@cha-suyeon/%EB%94%A5%EB%9F%AC%EB%8B%9D-Segmentation1-%EA%B0%9C%EB%85%90-%EC%9A%A9%EC%96%B4-%EC%A2%85%EB%A5%98Semantic-Instance-segmentation">Figure reference site</a></p>

<p><br /></p>

<h4 id="ms-coco">MS-COCO</h4>

<ul>
  <li>tensorflow api 등 open source 패키지들은 MS-COCO를 이용한 pretrained model 제공</li>
  <li>ID가 부여되었지만 데이터셋이 없는 것이 존재</li>
  <li>bounding box의 경우 <span style="color:red">좌상단 좌표와 width, height</span>가 표기</li>
  <li>annotation file은 json file 하나로 구성되어있는데 one line 처리가 되어있음..</li>
</ul>

<blockquote>
  <p>json format의 경우 별도의 주석 기능을 지원하지 않음. 굳이 넣는다면 절대 사용하지 않을 키 값을 만들어서 지정해주기.</p>
</blockquote>

<ul>
  <li>이미지 하나에 여러 object들을 가지고 있어서 타 데이터 셋보다 난이도가 높은 데이터 제공한다는 장점이 있음.</li>
</ul>

<p><br /><br /></p>

<h2 id="6-overview-of-opencv">6. Overview of OpenCV</h2>

<h4 id="python-기반-주요-이미지-라이브러리">Python 기반 주요 이미지 라이브러리</h4>

<ul>
  <li>PIL</li>
  <li>처리 성능이 상대적으로 느림</li>
  <li>Scikit Image</li>
  <li>Scipy 확장, numpy 기반</li>
  <li>전반적인 컴퓨터 비전 기능 제공</li>
  <li>OpenCN</li>
  <li>open source 기반 최고의 인기 라이브러리</li>
  <li>컴퓨터 비전 기능 일반화에 기여</li>
  <li>가장 중요한 부분 : <span style="color:red">image load(imread)할 때 RGB기반이 아니라 BGR기반으로 메모리에 load</span></li>
</ul>

<p><br /></p>

<p><img src="/assets/images/CV/CherryBlossom.jpeg" alt="RGB" width="60%" /></p>

<p><br /></p>

<p><img src="/assets/images/CV/CherryBlossom-BGR.png" alt="BGR" width="70%" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">cv2</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="c1">#!/usr/bin/env python
</span>
<span class="n">img_array</span> <span class="o">=</span> <span class="n">cv2</span><span class="p">.</span><span class="n">imread</span><span class="p">(</span><span class="s">'figs/CherryBlossom.jpeg'</span><span class="p">)</span>
<span class="c1">#plt.imshow(img_array)
</span></code></pre></div></div>

<h4 id="image-resolution-fps-detection-correlation">Image resolution, FPS, detection correlation</h4>

<ul>
  <li>Resolution이 높으면 detection 성능은 좋으나 FPS가 낮음</li>
  <li>알고리즘 개선이 없다면 진보된 하드웨어가 필요…</li>
</ul>

<p><br /><br /></p>
